import os
import sys
import time
import logging
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
import subprocess
import pickle
import json
import pandas as pd
from typing import Optional, List, Dict, Any
from sqlalchemy import create_engine, Column, String, Integer, DateTime, Table, MetaData
from sqlalchemy.orm import sessionmaker

from src.bot.data_collector import OddsDataCollector # type: ignore
from src.bot.api_key_manager import APIKeyManager # type: ignore
from src.bot.adaptive_poller import AdaptivePoller # type: ignore

# Import new Telegram notifications module
from src.notifications.telegram_notifications import ( # type: ignore
    send_error_alert,
    send_startup_notification,
    send_shutdown_notification,
    send_telegram_message as send_telegram_msg,
    send_backup_notification,
    send_backup_cleanup_notification
)

# Import backup manager functions
from src.bot.backup_manager import backup_daily, BackupManager # type: ignore

# === CONFIGURATION ===
load_dotenv('config/.env')

ODDS_API_KEYS = os.getenv("ODDS_API_KEYS", "")
KEY_LIST = [k.strip() for k in ODDS_API_KEYS.split(",") if k.strip()]
MAX_API_CALLS = int(os.getenv("MAX_API_CALLS", 500))
demo_max_calls = int(os.getenv("demo_max_calls", 2000))
DEMO_PHASE_ENABLED = os.getenv("DEMO_PHASE_ENABLED", "0") == "1"

api_key_mgr = APIKeyManager(
    max_calls=MAX_API_CALLS,
    demo_max_calls=demo_max_calls,
    demo_phase_enabled=DEMO_PHASE_ENABLED
)

ADVANCE_MINUTES = int(os.getenv("ADVANCE_MINUTES", "20"))
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
MIN_API_INTERVAL = float(os.getenv("MIN_API_INTERVAL", "2"))
POST_RUN_SLEEP = float(os.getenv("SCHEDULER_SLEEP_SECONDS", "900"))
RETRY_ON_ERROR = int(os.getenv("SCHEDULER_RETRY_ON_ERROR", "1"))
EVENT_WINDOW_HOURS = float(os.getenv("EVENT_WINDOW_HOURS", 6))
ODDS_CACHE_EXPIRE_MINUTES = float(os.getenv("ODDS_CACHE_EXPIRE_MINUTES", 10))
ODDS_CACHE_FILE = os.getenv("ODDS_CACHE_FILE", "data/odds_cache.json")
MARKET_ANALYTICS_FILE = os.getenv("MARKET_EDGE_FILE", "data/market_edge_summary.csv")
MARKETS_TO_SCAN_DEFAULT = [m.strip() for m in os.getenv("MARKETS", "h2h").split(",") if m.strip()]
BOOKMAKERS_STR = ",".join([b.strip() for b in os.getenv("BOOKMAKERS", "").split(",") if b.strip()])

SPORTS_TO_SCAN_ENV = os.getenv("SPORTS_TO_SCAN", "")
if SPORTS_TO_SCAN_ENV:
    SPORT_WHITELIST = set([s.strip() for s in SPORTS_TO_SCAN_ENV.split(",") if s.strip()])
else:
    SPORT_WHITELIST = set([
        "basketball_nba", "icehockey_nhl", "americanfootball_nfl",
        "americanfootball_ncaaf", "basketball_ncaab"
    ])

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")
SCHEDULER_LOG_FILE = os.getenv("SCHEDULER_LOG_FILE", "scheduling/scheduler_log.csv")
CACHE_FILE = os.getenv("SCHEDULER_CACHE_FILE", "scheduling/scheduler_cache.pkl")
REPORTING_FILE = os.getenv("BET_HISTORY_FILE", "data/bet_history.csv")
MANUAL_PNL_FILE = os.getenv("MANUAL_PNL_FILE", "data/manual_pnl.csv")
DATA_DIR = os.getenv("MANUAL_PNL_FILE", "manual_pnl.csv")  # ‚úÖ Just filename
ROI_TUNE_THRESHOLD = float(os.getenv("ROI_TUNE_THRESHOLD", 15))
MIN_SLEEP_INTERVAL = int(os.getenv("MIN_SLEEP_INTERVAL", 300))
MAX_SLEEP_INTERVAL = int(os.getenv("MAX_SLEEP_INTERVAL", 10800))  # 3 hours max
HEARTBEAT_INTERVAL = int(os.getenv("HEARTBEAT_INTERVAL", 300))  # 5 minutes
HEARTBEAT_FILE = os.getenv("HEARTBEAT_FILE", "logs/scheduler_heartbeat.txt")
BOT_VERSION = os.getenv("BOT_VERSION", "2.0.0")

# Adaptive polling configuration
ADAPTIVE_SCHEDULING = os.getenv("ADAPTIVE_SCHEDULING", "1") == "1"
BASE_RUN_INTERVAL = int(os.getenv("BASE_RUN_INTERVAL", "120"))
PEAK_HOURS_START = int(os.getenv("PEAK_HOURS_START", "17"))
PEAK_HOURS_END = int(os.getenv("PEAK_HOURS_END", "23"))
OFF_PEAK_MULTIPLIER = float(os.getenv("OFF_PEAK_MULTIPLIER", "3.0"))
QUOTA_WARNING_THRESHOLD = float(os.getenv("QUOTA_WARNING_THRESHOLD", "0.7"))
QUOTA_CRITICAL_THRESHOLD = float(os.getenv("QUOTA_CRITICAL_THRESHOLD", "0.9"))

# Sleep mode configuration
ENABLE_SLEEP_MODE = os.getenv("ENABLE_SLEEP_MODE", "0") == "1"
SLEEP_HOURS = os.getenv("SLEEP_HOURS", "0-8")

DB_URL = os.getenv("SCHEDULER_DB_URL", "sqlite:///scheduling/scheduler.db")
engine = create_engine(DB_URL)
metadata = MetaData()
scheduler_events = Table('scheduler_events', metadata,
    Column('id', Integer, primary_key=True, autoincrement=True),
    Column('timestamp', DateTime),
    Column('event', String),
    Column('details', String)
)
metadata.create_all(engine)
Session = sessionmaker(bind=engine)

logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler('logs/scheduler.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

_cache = {'active_sports': (None, 0), 'next_event': (None, 0)}
_odds_cache = {}
_last_heartbeat = 0
_scheduler_start_time = None
_adaptive_poller = None
_last_daily_backup = None  # Track last daily backup
_sleep_notified = False  # Track if we've sent sleep notification

def load_manual_pnl_analyzer():
    """Load manual P&L analyzer for adaptive polling."""
    try:
        # Try to import ManualPnLAnalyzer from src.bot.main
        try:
            from src.bot.main import ManualPnLAnalyzer # type: ignore
        except ImportError:
            logger.warning("‚ùå ManualPnLAnalyzer not found in bot.main - adaptive poller will use default priorities")
            return None
        
        manual_pnl_path = os.path.join(DATA_DIR, MANUAL_PNL_FILE)
        
        # Check if file exists
        if not os.path.exists(manual_pnl_path):
            logger.info(f"‚ÑπÔ∏è Manual P&L file not found: {manual_pnl_path} - using default priorities")
            return None
        
        # Try to load analyzer
        analyzer = ManualPnLAnalyzer(manual_pnl_path)
        
        # Verify it has required attributes
        if hasattr(analyzer, 'insights') and isinstance(analyzer.insights, dict):
            total_bets = analyzer.insights.get('total_bets', 0)
            logger.info(f"‚úÖ Loaded manual P&L analyzer: {total_bets} historical bets")
            return analyzer
        else:
            logger.warning("‚ö†Ô∏è ManualPnLAnalyzer loaded but missing expected 'insights' attribute")
            return None
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not load manual P&L analyzer: {e}")
        return None

def perform_daily_backup() -> None:
    """
    Perform daily backup at midnight and cleanup old backups.
    This function should be called periodically by the scheduler.
    """
    global _last_daily_backup
    
    now = datetime.now()
    
    # Check if it's midnight hour (00:00-00:59) and we haven't backed up today
    if now.hour == 0:
        # Check if we already did backup today
        today_date = now.date()
        if _last_daily_backup is None or _last_daily_backup.date() != today_date:
            logger.info("\n" + "=" * 70)
            logger.info("üíæ PERFORMING DAILY BACKUP AT MIDNIGHT")
            logger.info("=" * 70)
            
            try:
                # Create daily backup
                backup_path = backup_daily()
                
                if backup_path:
                    backup_size_mb = os.path.getsize(backup_path) / (1024 * 1024)
                    logger.info(f"‚úÖ Daily backup created: {backup_path} ({backup_size_mb:.2f} MB)")
                    
                    # Send backup notification
                    send_backup_notification(backup_path, "daily", backup_size_mb)
                    
                    # Perform cleanup
                    logger.info("üßπ Running backup cleanup...")
                    manager = BackupManager()
                    cleanup_stats = manager.cleanup_old_backups()
                    
                    logger.info(f"Cleanup complete: {cleanup_stats['deleted']} backups deleted, {cleanup_stats['freed_mb']:.2f} MB freed")
                    
                    # Send cleanup notification if anything was deleted
                    if cleanup_stats['deleted'] > 0:
                        send_backup_cleanup_notification(cleanup_stats)
                    
                    # Update last backup time
                    _last_daily_backup = now
                    
                    log_scheduler_event("DAILY_BACKUP_SUCCESS", f"Created backup and cleaned up {cleanup_stats['deleted']} old backups")
                else:
                    logger.warning("‚ö†Ô∏è Daily backup failed (non-critical)")
                    log_scheduler_event("DAILY_BACKUP_FAILED", "Backup creation returned None")
            
            except Exception as e:
                logger.error(f"‚ùå Daily backup error: {e}", exc_info=True)
                send_error_alert("Daily Backup", str(e)[:200], "warning")
                log_scheduler_event("DAILY_BACKUP_ERROR", str(e))
            
            logger.info("=" * 70)

def healthcheck_heartbeat() -> None:
    """Update heartbeat file for external monitoring."""
    global _last_heartbeat
    now = time.time()
    
    if now - _last_heartbeat < HEARTBEAT_INTERVAL:
        return
    
    try:
        uptime_seconds = now - _scheduler_start_time if _scheduler_start_time else 0
        uptime_hours = uptime_seconds / 3600
        
        # Get adaptive poller status if available
        polling_status = {}
        if _adaptive_poller:
            polling_status = _adaptive_poller.get_polling_summary()
        
        # Ensure logs directory exists
        os.makedirs('logs', exist_ok=True)
        
        with open(HEARTBEAT_FILE, 'w') as f:
            f.write(json.dumps({
                "timestamp": datetime.now().isoformat(),
                "total_api_calls": api_key_mgr.total_calls,
                "demo_phase": api_key_mgr.demo_phase_enabled,
                "status": "sleeping" if (_adaptive_poller and _adaptive_poller.is_sleep_hours()) else "running",
                "uptime_hours": round(uptime_hours, 2),
                "adaptive_polling": polling_status,
                "last_daily_backup": _last_daily_backup.isoformat() if _last_daily_backup else None
            }, indent=2))
        
        _last_heartbeat = now
        
        # Different log based on sleep status
        if _adaptive_poller and _adaptive_poller.is_sleep_hours():
            sleep_status = _adaptive_poller.get_sleep_status()
            logger.info(f"[HEARTBEAT] üí§ Sleeping - Wake at {sleep_status['wake_time']}, API calls: {api_key_mgr.total_calls}")
        else:
            logger.info(f"[HEARTBEAT] ‚òÄÔ∏è Active - Uptime: {uptime_hours:.1f}h, API calls: {api_key_mgr.total_calls}")
            
            if _adaptive_poller:
                logger.info(f"[HEARTBEAT] Quota: {polling_status.get('quota_usage', 'N/A')}, Peak hours: {polling_status.get('is_peak_hours', 'N/A')}")
    
    except Exception as e:
        logger.error(f"Heartbeat file update failed: {e}")

def demo_cap_reached() -> bool:
    """
    Check if demo API cap has been reached and exit if so.
    
    Returns:
        True if cap reached, False otherwise
    """
    if api_key_mgr.is_demo_cap_reached():
        msg = f"Demo API cap reached: {api_key_mgr.total_calls}/{api_key_mgr.demo_max_calls}. Scheduler will exit."
        logger.warning(msg)
        
        send_error_alert(
            "API Quota Exhausted",
            f"Demo cap reached: {api_key_mgr.total_calls}/{api_key_mgr.demo_max_calls}",
            "critical"
        )
        
        log_scheduler_event("DEMO_API_CAP_HIT", msg)
        
        send_shutdown_notification(
            "Demo API cap reached",
            {
                'opportunities': 0,
                'total_profit': 0,
                'uptime': f"{(time.time() - _scheduler_start_time) / 3600:.1f}h" if _scheduler_start_time else "N/A"
            }
        )
        
        sys.exit(0)
    return False

def log_scheduler_event(event: str, details: str = "") -> None:
    """
    Log scheduler event to both CSV and database.
    
    Args:
        event: Event name
        details: Event details
    """
    import csv
    
    headers = ["timestamp", "event", "details"]
    
    # Ensure directory exists
    os.makedirs(os.path.dirname(SCHEDULER_LOG_FILE), exist_ok=True)
    
    file_exists = os.path.isfile(SCHEDULER_LOG_FILE)
    
    try:
        # CSV logging
        with open(SCHEDULER_LOG_FILE, "a", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            if not file_exists:
                writer.writeheader()
            writer.writerow({
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "event": event,
                "details": details
            })
        
        # Database logging
        with Session() as session:
            session.execute(scheduler_events.insert().values(
                timestamp=datetime.now(),
                event=event,
                details=details
            ))
            session.commit()
        
        logger.debug(f"Event logged: {event}")
    
    except Exception as e:
        logger.error(f"Error logging scheduler event: {e}")

def load_cache() -> None:
    """Load persistent cache from disk."""
    global _cache
    
    if os.path.isfile(CACHE_FILE):
        try:
            with open(CACHE_FILE, "rb") as f:
                _cache.update(pickle.load(f))
            logger.info("Loaded persistent cache from disk")
        except Exception as e:
            logger.error(f"Error loading cache: {e}")

def save_cache() -> None:
    """Save persistent cache to disk."""
    try:
        # Ensure directory exists
        os.makedirs(os.path.dirname(CACHE_FILE), exist_ok=True)
        
        with open(CACHE_FILE, "wb") as f:
            pickle.dump(_cache, f)
        logger.debug("Saved persistent cache to disk")
    except Exception as e:
        logger.error(f"Error saving cache: {e}")

def load_odds_cache() -> None:
    """Load persistent odds cache from disk."""
    global _odds_cache
    
    if os.path.isfile(ODDS_CACHE_FILE):
        try:
            with open(ODDS_CACHE_FILE, "r") as f:
                _odds_cache.update(json.load(f))
            logger.info("Loaded persistent odds cache")
        except Exception as e:
            logger.error(f"Error loading odds cache: {e}")

def save_odds_cache() -> None:
    """Save persistent odds cache to disk."""
    try:
        # Ensure directory exists
        os.makedirs(os.path.dirname(ODDS_CACHE_FILE), exist_ok=True)
        
        with open(ODDS_CACHE_FILE, "w") as f:
            json.dump(_odds_cache, f, indent=2)
        logger.debug("Saved odds cache to disk")
    except Exception as e:
        logger.error(f"Error saving odds cache: {e}")

def select_best_markets(threshold: float = 0) -> List[str]:
    """
    Prioritize scan order by highest recent profit/edge.
    
    Args:
        threshold: Minimum profit threshold
        
    Returns:
        List of market names sorted by profitability
    """
    try:
        if not os.path.exists(MARKET_ANALYTICS_FILE):
            logger.debug("Market analytics file not found, using defaults")
            return MARKETS_TO_SCAN_DEFAULT
        
        df = pd.read_csv(MARKET_ANALYTICS_FILE)
        best = df[df["total_profit"] > threshold]
        
        if not best.empty:
            markets = best.sort_values("total_profit", ascending=False)["market"].tolist()
            logger.info(f"Selected {len(markets)} profitable markets")
            return markets
        
        markets = df.sort_values("total_profit", ascending=False)["market"].tolist()
        logger.info(f"Using all {len(markets)} markets (none above threshold)")
        return markets if markets else MARKETS_TO_SCAN_DEFAULT
    
    except Exception as e:
        logger.error(f"Market analytics error: {e}")
        return MARKETS_TO_SCAN_DEFAULT

def should_run_now_adaptive() -> tuple:
    """
    Determine if bot should run now using adaptive logic.
    
    Returns:
        Tuple of (should_run: bool, reason: str)
    """
    if not ADAPTIVE_SCHEDULING or not _adaptive_poller:
        return True, "Adaptive scheduling disabled"
    
    # Check sleep mode FIRST
    if _adaptive_poller.is_sleep_hours():
        sleep_status = _adaptive_poller.get_sleep_status()
        return False, f"üí§ Sleep mode active until {sleep_status['wake_time']}"
    
    # Check quota
    usage = _adaptive_poller.get_quota_usage_ratio()
    
    if usage >= 1.0:
        return False, f"Quota exhausted: {usage:.1%}"
    
    # Critical quota - only run during peak hours
    if usage >= QUOTA_CRITICAL_THRESHOLD:
        if _adaptive_poller.is_peak_hours():
            return True, f"Critical quota ({usage:.1%}) but peak hours - proceeding cautiously"
        else:
            return False, f"Critical quota ({usage:.1%}) during off-peak - skipping"
    
    # Warning quota - reduce frequency
    if usage >= QUOTA_WARNING_THRESHOLD:
        # Use time-based heuristic
        current_minute = datetime.now().minute
        if current_minute % 2 == 0:  # Run every other scheduled time
            return True, f"Warning quota ({usage:.1%}) - reduced frequency"
        else:
            return False, f"Warning quota ({usage:.1%}) - skipping this run"
    
    # Normal operation
    return True, f"Normal operation (quota: {usage:.1%})"

def get_dynamic_sleep_interval() -> int:
    """
    Calculate dynamic sleep interval based on ROI, time of day, API usage, adaptive polling, and sleep mode.
    
    Returns:
        Sleep interval in seconds
    """
    # Check sleep mode first
    if ADAPTIVE_SCHEDULING and _adaptive_poller and _adaptive_poller.is_sleep_hours():
        # During sleep hours, check every 5 minutes for wake-up time
        return 300
    
    # Use adaptive poller if available
    if ADAPTIVE_SCHEDULING and _adaptive_poller:
        # Get base adaptive interval
        # For scheduler-level, we average across all sports
        sports = list(SPORT_WHITELIST)
        if sports:
            intervals = [_adaptive_poller.get_adaptive_interval(sport) for sport in sports]
            avg_interval = sum(intervals) / len(intervals)
            logger.info(f"üìä Adaptive average interval: {avg_interval:.0f}s ({avg_interval/60:.1f} min)")
            logger.info(f"   Peak hours: {_adaptive_poller.is_peak_hours()}")
            return int(avg_interval)
    
    # Fallback to original logic
    now = datetime.now(timezone.utc)
    hour = now.hour
    
    # Time-based default interval
    if 10 <= hour < 22:  # Peak hours
        default_interval = 15 * 60
    elif hour >= 22 or hour < 4:  # Late night
        default_interval = 30 * 60
    else:  # Off-peak
        default_interval = 60 * 60
    
    # ROI-based adjustment
    roi_by_sport = get_recent_roi_by_sport()
    high_roi_found = any(roi >= ROI_TUNE_THRESHOLD for roi in roi_by_sport.values())
    
    if high_roi_found:
        logger.info(f"High ROI detected: {roi_by_sport}. Using aggressive interval: {MIN_SLEEP_INTERVAL}s")
        return MIN_SLEEP_INTERVAL
    
    # API usage-based adjustment
    if api_key_mgr.demo_phase_enabled:
        usage_pct = api_key_mgr.total_calls / api_key_mgr.demo_max_calls
        if usage_pct > 0.9:
            adjusted = min(default_interval * 2, MAX_SLEEP_INTERVAL)
            logger.info(f"API usage at {usage_pct*100:.1f}%, increasing interval to {adjusted}s")
            return adjusted
    
    return default_interval

def get_recent_roi_by_sport(report_file: str = REPORTING_FILE, hours: int = 24) -> Dict[str, float]:
    """
    Calculate recent ROI broken down by sport.
    
    Args:
        report_file: Path to bet history file
        hours: Number of hours to look back
        
    Returns:
        Dictionary mapping sport to ROI percentage
    """
    try:
        if not os.path.exists(report_file):
            return {}
        
        df = pd.read_csv(report_file, parse_dates=["timestamp"])
        cutoff_time = datetime.now() - timedelta(hours=hours)
        df = df[df["timestamp"] > cutoff_time]
        
        if df.empty:
            return {}
        
        roi_by_sport = {}
        start_bankroll = float(os.getenv("START_BANKROLL", 100))
        
        for sport in df["sport"].unique():
            sport_df = df[df["sport"] == sport]
            profit = sport_df["profit"].sum()
            roi = (profit / start_bankroll) * 100 if start_bankroll > 0 else 0
            roi_by_sport[sport] = round(roi, 2)
        
        return roi_by_sport
    
    except Exception as e:
        logger.error(f"ROI calculation error: {e}")
        return {}

def run_bot_with_key(api_key: str) -> bool:
    """
    Execute main arbitrage bot with specified API key.
    
    Args:
        api_key: API key to use for bot execution
        
    Returns:
        True if successful, False otherwise
    """
    if demo_cap_reached():
        return False
    
    # Check adaptive run criteria
    should_run, reason = should_run_now_adaptive()
    if not should_run:
        logger.info(f"‚è≠Ô∏è Skipping bot run: {reason}")
        log_scheduler_event("BOT_RUN_SKIPPED", reason)
        return True  # Return True to avoid retry logic
    
    os.environ["ODDS_API_KEY"] = api_key
    now_utc = datetime.now(timezone.utc)
    
    logger.info(f"Running arbitrage bot with API key ...{api_key[-5:]} at {now_utc.isoformat()}")
    logger.info(f"Adaptive decision: {reason}")
    log_scheduler_event("BOT_START", f"Key: ...{api_key[-5:]}, Reason: {reason}")
    
    try:
        # CRITICAL FIX: Use module execution instead of direct file call
        # Set PYTHONPATH for subprocess
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        env = os.environ.copy()
        env['PYTHONPATH'] = project_root

        result = subprocess.run(
            [sys.executable, '-m', 'src.bot.main'],
            capture_output=True,
            text=True,
            timeout=3600,
            cwd=project_root,  # Run from project root
            env=env  # Include PYTHONPATH
        )
        
        logger.info(f"Bot stdout:\n{result.stdout}")
        if result.stderr:
            logger.error(f"Bot stderr:\n{result.stderr}")
            send_error_alert("Bot Execution Error", result.stderr[:400], "error")
        
        logger.info(f"Bot exited with code {result.returncode}")
        log_scheduler_event("BOT_FINISH", f"Code: {result.returncode}")
        
        if result.returncode == 0:
            send_telegram_msg("‚úÖ <b>Bot completed successfully</b>")
            
            # Send ROI summary
            try:
                if os.path.exists(REPORTING_FILE):
                    df = pd.read_csv(REPORTING_FILE)
                    total_profit = df["profit"].sum()
                    start_bankroll = float(os.getenv("START_BANKROLL", 100))
                    roi = (total_profit / start_bankroll) * 100 if start_bankroll > 0 else 0
                    
                    summary_msg = f"""
üìà <b>Latest Performance</b>

üí∞ Total Profit: ${total_profit:.2f}
üìä ROI: {roi:.2f}%
üé≤ Total Bets: {len(df)}
"""
                    send_telegram_msg(summary_msg)
            except Exception as e:
                logger.warning(f"Could not compute ROI summary: {e}")
            
            return True
        else:
            send_error_alert("Bot Execution", f"Bot exited with code {result.returncode}", "error")
            
            if RETRY_ON_ERROR:
                logger.info("Retrying bot after error with backoff...")
                time.sleep(60)
                return run_bot_with_key(api_key_mgr.get_most_available_key())
            return False
    
    except subprocess.TimeoutExpired:
        logger.error("Bot execution timed out after 600s")
        send_error_alert("Bot Timeout", "Bot execution timed out after 600 seconds", "critical")
        return False
    
    except Exception as e:
        logger.error(f"Bot execution error: {e}")
        send_error_alert("Bot Execution", str(e)[:200], "critical")
        return False

def get_active_sports() -> List[str]:
    """
    Fetch list of currently active sports with caching and adaptive prioritization.
    
    Returns:
        List of active sport keys, prioritized by adaptive poller
    """
    now_ts = time.time()
    sports, expiry = _cache['active_sports']
    
    if sports and now_ts < expiry:
        logger.debug("Using cached active sports list")
        # Apply adaptive prioritization if available
        if _adaptive_poller:
            return _adaptive_poller.get_prioritized_sports(sports)
        return sports
    
    collector = OddsDataCollector(api_key_manager=api_key_mgr, logger=logger)
    
    try:
        active_keys = collector.fetch_sports()
        filtered = [s for s in active_keys if s in SPORT_WHITELIST]
        logger.info(f"Active in-season sports: {filtered}")
        _cache['active_sports'] = (filtered, now_ts + 30 * 60)
        save_cache()
        
        # Apply adaptive prioritization
        if _adaptive_poller:
            return _adaptive_poller.get_prioritized_sports(filtered)
        return filtered
    
    except Exception as e:
        logger.error(f"Could not fetch active sports: {e}")
        send_error_alert("Sports Fetch", str(e), "warning")
        sports_list = list(SPORT_WHITELIST)
        if _adaptive_poller:
            return _adaptive_poller.get_prioritized_sports(sports_list)
        return sports_list

def get_next_event_time(
    sports_to_scan: List[str],
    bookmakers_str: str,
    markets_to_scan: List[str],
    min_interval: float = 2.0
) -> Optional[datetime]:
    """
    Find the next upcoming event time across all sports.
    
    Args:
        sports_to_scan: List of sport keys to check
        bookmakers_str: Comma-separated bookmakers
        markets_to_scan: List of markets to check
        min_interval: Minimum interval between API calls
        
    Returns:
        Datetime of next event, or None if no events found
    """
    now_ts = time.time()
    next_event, expiry = _cache['next_event']
    
    if next_event and now_ts < expiry:
        logger.debug("Using cached next event time")
        return next_event
    
    collector = OddsDataCollector(api_key_manager=api_key_mgr, logger=logger)
    markets_str = ",".join(markets_to_scan)
    event_candidates = []
    last_call = 0
    now_utc = datetime.now(timezone.utc)
    
    load_odds_cache()
    
    for sport in sports_to_scan:
        # Adaptive rate limiting
        if _adaptive_poller:
            # Check if should poll this sport
            if not _adaptive_poller.should_poll_sport(sport):
                logger.info(f"‚è≠Ô∏è Skipping {sport} - quota preservation")
                continue
            
            # Use adaptive interval
            adaptive_interval = _adaptive_poller.get_adaptive_interval(sport) / len(sports_to_scan)
            wait_time = max(min_interval, adaptive_interval)
        else:
            wait_time = min_interval
        
        # Rate limiting
        now = time.time()
        elapsed = now - last_call
        if elapsed < wait_time:
            time.sleep(wait_time - elapsed)
        last_call = time.time()
        
        try:
            games = collector.fetch_odds(sport, bookmakers=bookmakers_str, markets=markets_str)
        except Exception as e:
            logger.error(f"Error fetching odds for {sport}: {e}")
            continue
        
        for game in games:
            start_str = game.get("commence_time")
            if not start_str:
                continue
            
            try:
                start_dt = datetime.fromisoformat(start_str.replace("Z", "+00:00"))
                
                # Skip past events and events too far in future
                if start_dt <= now_utc or (start_dt - now_utc).total_seconds() > EVENT_WINDOW_HOURS * 3600:
                    continue
            
            except Exception:
                logger.warning(f"Could not parse start time: {start_str}")
                continue
            
            # Cache odds data
            evt_id = str(game.get("id"))
            cache_expiry = _odds_cache.get(evt_id, {}).get("expiry", 0)
            
            if now_ts < cache_expiry:
                odds_data = _odds_cache[evt_id]["odds"]
            else:
                odds_data = game.get("bookmakers", [])
                _odds_cache[evt_id] = {
                    "odds": odds_data,
                    "expiry": now_ts + ODDS_CACHE_EXPIRE_MINUTES * 60
                }
            
            event_candidates.append((start_dt, (start_dt - now_utc).total_seconds()))
    
    save_odds_cache()
    logger.info(f"Found {len(event_candidates)} upcoming events")
    
    # Sort by time until event
    event_candidates.sort(key=lambda x: (x[1], x[0]))
    soonest = event_candidates[0][0] if event_candidates else None
    
    if soonest:
        _cache['next_event'] = (soonest, now_ts + 10 * 60)
    else:
        _cache['next_event'] = (None, now_ts + 10 * 60)
    
    save_cache()
    return soonest

def reload_config() -> None:
    """Hot reload .env configuration without restart."""
    load_dotenv('config/.env', override=True)
    logger.info("Config hot-reloaded from config/.env")

def dynamic_scheduler() -> None:
    """Main scheduler loop with event-driven execution, adaptive polling, and sleep mode."""
    global _scheduler_start_time, _adaptive_poller, _sleep_notified
    _scheduler_start_time = time.time()
    
    logger.info("=" * 70)
    logger.info("Starting dynamic event-driven scheduler with adaptive polling and sleep mode")
    logger.info(f"Demo phase: {api_key_mgr.demo_phase_enabled}")
    logger.info(f"API cap: {api_key_mgr.demo_max_calls if api_key_mgr.demo_phase_enabled else api_key_mgr.max_calls}")
    logger.info(f"Adaptive scheduling: {ADAPTIVE_SCHEDULING}")
    logger.info(f"Sleep mode: {ENABLE_SLEEP_MODE} ({SLEEP_HOURS if ENABLE_SLEEP_MODE else 'disabled'})")
    logger.info("=" * 70)
    
    # Initialize adaptive poller
    if ADAPTIVE_SCHEDULING:
        logger.info("\n‚öôÔ∏è Initializing adaptive polling system...")
        manual_pnl_analyzer = load_manual_pnl_analyzer()
        
        _adaptive_poller = AdaptivePoller(
            api_key_manager=api_key_mgr,
            manual_pnl_analyzer=manual_pnl_analyzer,
            base_interval=BASE_RUN_INTERVAL,
            peak_hours=(PEAK_HOURS_START, PEAK_HOURS_END),
            off_peak_multiplier=OFF_PEAK_MULTIPLIER,
            quota_threshold_warning=QUOTA_WARNING_THRESHOLD,
            quota_threshold_critical=QUOTA_CRITICAL_THRESHOLD
        )
        
        status = _adaptive_poller.get_polling_summary()
        logger.info(f"üìä Adaptive Polling Status:")
        logger.info(f"   Peak hours: {status['is_peak_hours']}")
        logger.info(f"   Sleep mode: {status['sleep_mode']}")
        logger.info(f"   Quota usage: {status['quota_usage']}")
        logger.info(f"   Time multiplier: {status['time_multiplier']:.1f}x")
        logger.info(f"   Quota multiplier: {status['quota_multiplier']:.1f}x")
        logger.info("=" * 70)
    
    # Send startup notification
    send_startup_notification(
        BOT_VERSION,
        {
            'simulate': True,
            'bankroll': float(os.getenv("START_BANKROLL", 100)),
            'min_margin': float(os.getenv("MIN_MARGIN", 0.01)),
            'sports': list(SPORT_WHITELIST),
            'markets': MARKETS_TO_SCAN_DEFAULT,
            'adaptive_polling': ADAPTIVE_SCHEDULING,
            'sleep_mode': ENABLE_SLEEP_MODE,
            'sleep_hours': SLEEP_HOURS if ENABLE_SLEEP_MODE else None
        }
    )
    
    load_cache()
    run_count = 0
    skip_count = 0
    consecutive_errors = 0
    
    while True:
        try:
            healthcheck_heartbeat()
            demo_cap_reached()
            
            # ===== SLEEP MODE CHECK (FIRST THING IN LOOP) =====
            if _adaptive_poller and _adaptive_poller.is_sleep_hours():
                if not _sleep_notified:
                    sleep_status = _adaptive_poller.get_sleep_status()
                    logger.info(f"üí§ Entering sleep mode until {sleep_status['wake_time']}")
                    log_scheduler_event("SLEEP_MODE_ACTIVE", f"Sleeping until {sleep_status['wake_time']}")
                    
                    # Send Telegram notification
                    send_telegram_msg(
                        f"üí§ <b>BOT SLEEPING</b>\n\n"
                        f"Sleep mode active until {sleep_status['wake_time']}\n"
                        f"No API calls during this period.\n\n"
                        f"Current API usage: {api_key_mgr.total_calls}/{api_key_mgr.demo_max_calls}"
                    )
                    
                    _sleep_notified = True
                
                # Sleep for 5 minutes, then check again
                time.sleep(300)
                continue
            else:
                # Reset sleep notification flag when awake
                if _sleep_notified:
                    logger.info("‚òÄÔ∏è Waking up - resuming normal operations")
                    log_scheduler_event("SLEEP_MODE_ENDED", "Resumed operations")
                    
                    send_telegram_msg(
                        f"‚òÄÔ∏è <b>BOT AWAKE</b>\n\n"
                        f"Sleep mode ended - resuming operations\n"
                        f"Current API usage: {api_key_mgr.total_calls}/{api_key_mgr.demo_max_calls}"
                    )
                    
                    _sleep_notified = False
            
            # ===== REST OF YOUR EXISTING SCHEDULER LOGIC =====
            
            # Check for daily backup at midnight
            perform_daily_backup()
            
            sleep_stretch = get_dynamic_sleep_interval()
            markets_to_scan = select_best_markets()
            sports_to_scan = get_active_sports()
            
            if not sports_to_scan:
                logger.info("No active sports - sleeping longer")
                log_scheduler_event("NO_ACTIVE_SPORTS", f"Sleeping {sleep_stretch}s")
                time.sleep(sleep_stretch)
                continue
            
            next_event = get_next_event_time(
                sports_to_scan,
                BOOKMAKERS_STR,
                markets_to_scan,
                min_interval=MIN_API_INTERVAL
            )
            
            now_utc = datetime.now(timezone.utc)
            
            # No upcoming events
            if not next_event or next_event < now_utc:
                sleep_stretch = min(sleep_stretch * 2, MAX_SLEEP_INTERVAL)
                logger.info(f"No upcoming events. Sleeping {sleep_stretch//60} minutes")
                log_scheduler_event("NO_NEXT_EVENT", f"Sleeping {sleep_stretch//60} min")
                time.sleep(sleep_stretch)
                continue
            
            # Calculate run time
            run_time = next_event - timedelta(minutes=ADVANCE_MINUTES)
            delay = (run_time - now_utc).total_seconds()
            
            if delay <= 0:
                # Run now
                logger.info(f"Optimal window is now (event at {next_event}). Running bot")
                log_scheduler_event("RUN_NOW", f"Event at {next_event}")
                send_telegram_msg(f"üéØ Running bot now for event at {next_event.strftime('%Y-%m-%d %H:%M:%S')} UTC")
                
                success = run_bot_with_key(api_key_mgr.get_most_available_key())
                
                if success:
                    consecutive_errors = 0
                    run_count += 1
                else:
                    consecutive_errors += 1
                    skip_count += 1
                    if consecutive_errors >= 3:
                        logger.error("3 consecutive bot failures - alerting")
                        send_error_alert("Bot Failures", "3 consecutive bot failures detected", "critical")
                
                reload_config()
                log_scheduler_event("SLEEP_AFTER_RUN", f"Sleeping {POST_RUN_SLEEP}s")
                time.sleep(POST_RUN_SLEEP)
            
            else:
                # Schedule for later
                logger.info(f"Next event at {next_event}. Bot will run in {delay/60:.1f} minutes")
                log_scheduler_event("SCHEDULED_SLEEP", f"Bot will run in {delay/60:.1f} min")
                send_telegram_msg(
                    f"‚è∞ Next bot run in {delay/60:.1f} min for event at {next_event.strftime('%Y-%m-%d %H:%M:%S')} UTC"
                )
                
                time.sleep(delay)
                
                success = run_bot_with_key(api_key_mgr.get_most_available_key())
                
                if success:
                    consecutive_errors = 0
                    run_count += 1
                else:
                    consecutive_errors += 1
                    skip_count += 1
                
                log_scheduler_event("SLEEP_AFTER_RUN", f"Sleeping {POST_RUN_SLEEP}s")
                time.sleep(POST_RUN_SLEEP)
            
            # Periodic maintenance and status report
            if (run_count + skip_count) % 10 == 0 or (time.time() - _scheduler_start_time > 86400):
                reload_config()
                uptime_hours = (time.time() - _scheduler_start_time) / 3600
                logger.info(f"\nüìä Scheduler Stats:")
                logger.info(f"   Completed runs: {run_count}")
                logger.info(f"   Skipped runs: {skip_count}")
                logger.info(f"   Total runtime: {uptime_hours:.1f}h")
                logger.info(f"   API calls: {api_key_mgr.total_calls}")
                logger.info(f"   Last daily backup: {_last_daily_backup or 'Never'}")
                
                if _adaptive_poller:
                    status = _adaptive_poller.get_polling_summary()
                    logger.info(f"   Quota usage: {status['quota_usage']}")
                    logger.info(f"   Peak hours: {status['is_peak_hours']}")
                    logger.info(f"   Sleep mode: {status['sleep_mode']}")
        
        except KeyboardInterrupt:
            logger.info("Scheduler stopped by user")
            send_shutdown_notification("User interrupted")
            log_scheduler_event("SCHEDULER_STOPPED", "User interrupted")
            sys.exit(0)
        
        except Exception as exc:
            logger.error(f"Unhandled exception: {exc}", exc_info=True)
            send_error_alert("Scheduler Critical Error", str(exc)[:200], "critical")
            log_scheduler_event("SCHEDULER_CRITICAL_ERROR", str(exc))
            time.sleep(60)
            continue

if __name__ == "__main__":
    try:
        config_log = {
            'ADVANCE_MINUTES': ADVANCE_MINUTES,
            'MIN_API_INTERVAL': MIN_API_INTERVAL,
            'POST_RUN_SLEEP': POST_RUN_SLEEP,
            'BOOKMAKERS_STR': BOOKMAKERS_STR,
            'SPORT_WHITELIST': list(SPORT_WHITELIST),
            'DEMO_PHASE': api_key_mgr.demo_phase_enabled,
            'ADAPTIVE_SCHEDULING': ADAPTIVE_SCHEDULING,
            'BASE_RUN_INTERVAL': BASE_RUN_INTERVAL,
            'PEAK_HOURS': f"{PEAK_HOURS_START}-{PEAK_HOURS_END}",
            'QUOTA_THRESHOLDS': f"Warning: {QUOTA_WARNING_THRESHOLD:.0%}, Critical: {QUOTA_CRITICAL_THRESHOLD:.0%}",
            'SLEEP_MODE': ENABLE_SLEEP_MODE,
            'SLEEP_HOURS': SLEEP_HOURS if ENABLE_SLEEP_MODE else None
        }
        
        logger.info("Scheduler startup: loaded config")
        logger.info(json.dumps(config_log, indent=2))
        log_scheduler_event("SCHEDULER_STARTUP", json.dumps(config_log))
        
        dynamic_scheduler()
    
    except Exception as e:
        logger.error(f"Fatal startup error: {e}", exc_info=True)
        send_error_alert("Scheduler Startup", str(e)[:200], "critical")
        sys.exit(1)

